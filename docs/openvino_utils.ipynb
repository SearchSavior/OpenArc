{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO Utilities Notebook\n",
    "\n",
    "Various utilities live in this notebook to help users of OpenArc understand the properties of their devices; mastering understanding of available data types, quantization strategies and  available optimization techniques is only one part of learning to use OpenVINO on different kinds of hardware.\n",
    "\n",
    "OpenArc does some of the work of serving inference but is opinionated in areas of the approach; OpenArc doesn't hand hold like other Intel applications like [Intel AI Playground](https://github.com/intel/AI-Playground) which are more entry-level-plug-and-play.\n",
    "\n",
    "Thanks again for checking out my project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to working with Intel Devices\n",
    "\n",
    "This document offers discussion of \"lessons-learned\" from months of working with Intel GPU devices; *hours* of blood, sweat, and tears went into setting up this project and it's a good place to share what I've learned. At this stage in the Intel AI Stack it seems like a neccessary contribution to the community.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to working with Intel Devices\n",
    "\n",
    "This document offers discussion of \"lessons-learned\" from months of working with Intel GPU devices; *hours* of blood, sweat, and tears went into setting up this project and it's a good place to share what I've learned. At this stage in the Intel AI Stack it seems like a neccessary contribution to the community.\n",
    "\n",
    "### What is OpenVINO?\n",
    "\n",
    "OpenVINO is an inference backend for *acclerating* inference deployments of machine learning models on Intel hardware. It can be hard to understand the documentation- the Intel AI stack has many staff engineers/contributors to all manner of areas in the open source ecosystem and much of the stack is evolving without massive community contributions like what we have seen with llama.cpp. \n",
    "\n",
    "Many reasons contribute to the decline of Intel's dominance/popularity in the hardware space in the past few years; however they offer extensive open source contributions to many areas of AI, ML and have been since before [Attention Is All You Need](https://arxiv.org/abs/1706.03762). AI didn't start in 2017- however the demand for faster inference on existing infrastructure has never been higher. Plus, Arc chips are cheap but come with a steep learning curve. Sure, you can settle for Vulkan... but you aren't here to download a GGUF and send it.  \n",
    "\n",
    "\n",
    "\n",
    "### OpenVINO Utilities\n",
    "\n",
    "Various utilities live in this notebook to help users of OpenArc understand the properties of their devices; mastering understanding of available data types, quantization strategies and  available optimization techniques is only one part of learning to use OpenVINO on different kinds of hardware.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Check out the [Guide to the OpenVINO IR] and then use my [Command Line Tool tool](https://huggingface.co/spaces/Echo9Zulu/Optimum-CLI-Tool_tool) to perform converion. There are default approachs that \"work\" but to really leverage available compute you have to dig deeper and convert models yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic: Device Query\n",
    "\n",
    "\n",
    "Reccomended usage strategies:\n",
    "    - Driver issues\n",
    "    - Device access permissions\n",
    "    - Test Hardware access from containers\n",
    "    - Python path visibility\n",
    "    - Proper environment variable configuration \n",
    "\n",
    "#### Example use cases:\n",
    "\n",
    "1. Evaluating conflicting dependencies\n",
    "    - With careful dependency management you can control hardware across the Intel AI stack.\n",
    "    - However \n",
    "\n",
    "\n",
    "2. Say you need to have PyTorch, IPEX and OpenVINO in one conda env.\n",
    "    - This test alongside an XPU device query creates useful diagnostic infomration. \n",
    "    - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic Device Query\n",
    "\n",
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "available_devices = core.available_devices\n",
    "\n",
    "print(available_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding your device: Device Query\n",
    "\n",
    "Working with OpenVINO requires understanding facts about your device.\n",
    "\n",
    "OpenVINO uses an Intermediate Representation format to translate a model graph into a proprietary format used by the C++ runtime. \n",
    "\n",
    "OpenArc takes the optimization process a step further by offering tools for converting models which embrace the complexity of the task. \n",
    "\n",
    "While the excellent CLI tool streamlines the process,  each parameter requires careful consideration of several different facts the Device Query makes easier to discover. Seriously- use [Intel Ark](https://www.intel.com/content/www/us/en/ark.html) for hardware you don't own and the Device Query for every other convieveable usecase.\n",
    "\n",
    "Here's what's most important to consider:\n",
    "\n",
    "### Supported Datatypes\n",
    "\n",
    "Most Intel Devices will support FP32 natives as well \n",
    "\n",
    "\n",
    "### Quantization\n",
    "\n",
    "The same rules, practices and \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Query: \n",
    "\n",
    "\n",
    "# Taken from https://github.com/openvinotoolkit/openvino/blob/master/samples/python/hello_query_device/hello_query_device.py\n",
    "\n",
    "import logging as log\n",
    "import sys\n",
    "\n",
    "import openvino as ov\n",
    "\n",
    "\n",
    "def param_to_string(parameters) -> str:\n",
    "    \"\"\"Convert a list / tuple of parameters returned from OV to a string.\"\"\"\n",
    "    if isinstance(parameters, (list, tuple)):\n",
    "        return ', '.join([str(x) for x in parameters])\n",
    "    else:\n",
    "        return str(parameters)\n",
    "\n",
    "\n",
    "def main():\n",
    "    log.basicConfig(format='[ %(levelname)s ] %(message)s', level=log.INFO, stream=sys.stdout)\n",
    "\n",
    "    # --------------------------- Step 1. Initialize OpenVINO Runtime Core --------------------------------------------\n",
    "    core = ov.Core()\n",
    "\n",
    "    # --------------------------- Step 2. Get metrics of available devices --------------------------------------------\n",
    "    log.info('Available devices:')\n",
    "    for device in core.available_devices:\n",
    "        log.info(f'{device} :')\n",
    "        log.info('\\tSUPPORTED_PROPERTIES:')\n",
    "        for property_key in core.get_property(device, 'SUPPORTED_PROPERTIES'):\n",
    "            if property_key not in ('SUPPORTED_PROPERTIES'):\n",
    "                try:\n",
    "                    property_val = core.get_property(device, property_key)\n",
    "                except TypeError:\n",
    "                    property_val = 'UNSUPPORTED TYPE'\n",
    "                log.info(f'\\t\\t{property_key}: {param_to_string(property_val)}')\n",
    "        log.info('')\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------------------\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.exit(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Conversion\n",
    "\n",
    "OpenVINO is an inference engine for leveraging diverse types of compute. To squeeze as much performance as possible from any hardware requires a bit more work than using the naive approach, especially once you have a usecase in mind and know what hardware you are using.\n",
    "\n",
    "### The Naive Approach\n",
    "\n",
    "OpenVINO defaults to **int8_asym** when setting \"export=True\" in both **OVModelForCausalLM.from_pretrained()** and the Optimum CLI Export Tool if no arguments for weight_format are passed. \n",
    "\n",
    "OpenArc has been designed for usecases which wander toward the bleeding edge of AI where users are expected to understand the nuance of datatypes, quantization strategies, calibration datasets, how these parameters contribute to accuracy loss and maybe have just come from IPEX or (as of 2.5) 'vanilla' Pytorch and are looking to optimize a deployment.\n",
    "\n",
    "For convience \"export=False\" is exposed on the /model/load endpoint; however I **strongly discourage** using it. To get the best performance from OpenVINO you have to get into the weeds.\n",
    "\n",
    "### The Less Naive Approach to Model Conversion\n",
    "\n",
    "Many Intel CPUs support INT8 but it isn't always the best choice. \n",
    "\n",
    "OpenVINO notebooks prove out that INT4 weight only compression coupled with quantization strategies like AWQ + Scale Estimation achieve better performance across the Intel device ecosystem with negligable accuracy loss. Still, different model architectures offer different performance reguardless of the chosen datatype; in practice it can be hard to predict how a model will perform so understanding how these parameter's work is essential to maximizing throughput by testing different configurations on the same target model.\n",
    "\n",
    "\n",
    "### Why Speed Matters\n",
    "\n",
    "Nvidia GPUs are faster and have a better open source backbone than Intel. However, Intel devices are cheaper by comparison. Even so, I don't want speed for the sake of being fast. OpenArc has been tooled for Agentic usecases and synthetic data generation where low throughput can damage workflow execution. \n",
    "\n",
    "If I want to dump some problem into a RoundRobin style multi-turn chat I am not sitting there waiting for \n",
    "\n",
    "\n",
    "\n",
    "Note: If you are using cloude compute it should still work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenVINO-Transformers-Chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
