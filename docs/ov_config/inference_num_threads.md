
inference_num_threads is the number of CPU cores that will be used for inference. 

Use **htop** or **hwinfo** to watch the CPU usage during inference and tinker with this number to increase throughput, lower latency for all types of requests

