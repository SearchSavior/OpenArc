{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "size must contain 'shortest_edge' and 'longest_edge' keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m ov_config = {\u001b[33m\"\u001b[39m\u001b[33mPERFORMANCE_HINT\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLATENCY\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     18\u001b[39m model = OVModelForVisualCausalLM.from_pretrained(model_id, export=\u001b[38;5;28;01mFalse\u001b[39;00m, device=\u001b[33m\"\u001b[39m\u001b[33mGPU.2\u001b[39m\u001b[33m\"\u001b[39m, ov_config=ov_config) \u001b[38;5;66;03m#trust_remote_code=True)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m processor = \u001b[43mAutoProcessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m image_path = \u001b[33m\"\u001b[39m\u001b[33mdedication.png\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m image = Image.open(image_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/OpenArc-Test/lib/python3.11/site-packages/transformers/models/auto/processing_auto.py:345\u001b[39m, in \u001b[36mAutoProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m processor_class.from_pretrained(\n\u001b[32m    342\u001b[39m         pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n\u001b[32m    343\u001b[39m     )\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m processor_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocessor_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[38;5;66;03m# Last try: we use the PROCESSOR_MAPPING.\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m PROCESSOR_MAPPING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/OpenArc-Test/lib/python3.11/site-packages/transformers/processing_utils.py:1070\u001b[39m, in \u001b[36mProcessorMixin.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[39m\n\u001b[32m   1067\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1068\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m] = token\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m args = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1071\u001b[39m processor_dict, kwargs = \u001b[38;5;28mcls\u001b[39m.get_processor_dict(pretrained_model_name_or_path, **kwargs)\n\u001b[32m   1072\u001b[39m processor_dict.update({k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m processor_dict.keys()})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/OpenArc-Test/lib/python3.11/site-packages/transformers/processing_utils.py:1134\u001b[39m, in \u001b[36mProcessorMixin._get_arguments_from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1131\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1132\u001b[39m         attribute_class = \u001b[38;5;28mcls\u001b[39m.get_possibly_dynamic_module(class_name)\n\u001b[32m-> \u001b[39m\u001b[32m1134\u001b[39m     args.append(\u001b[43mattribute_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/OpenArc-Test/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:557\u001b[39m, in \u001b[36mAutoImageProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m image_processor_class.from_dict(config_dict, **kwargs)\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m image_processor_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimage_processor_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# Last try: we use the IMAGE_PROCESSOR_MAPPING.\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m IMAGE_PROCESSOR_MAPPING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/OpenArc-Test/lib/python3.11/site-packages/transformers/image_processing_base.py:423\u001b[39m, in \u001b[36mImageProcessingMixin.from_dict\u001b[39m\u001b[34m(cls, image_processor_dict, **kwargs)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcrop_size\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcrop_size\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m image_processor_dict:\n\u001b[32m    421\u001b[39m     image_processor_dict[\u001b[33m\"\u001b[39m\u001b[33mcrop_size\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcrop_size\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m image_processor = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mimage_processor_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[38;5;66;03m# Update image_processor with kwargs if needed\u001b[39;00m\n\u001b[32m    426\u001b[39m to_remove = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/OpenArc-Test/lib/python3.11/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl.py:144\u001b[39m, in \u001b[36mQwen2VLImageProcessor.__init__\u001b[39m\u001b[34m(self, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, min_pixels, max_pixels, patch_size, temporal_patch_size, merge_size, **kwargs)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(**kwargs)\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mshortest_edge\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m size \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlongest_edge\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m size):\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msize must contain \u001b[39m\u001b[33m'\u001b[39m\u001b[33mshortest_edge\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlongest_edge\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keys.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    146\u001b[39m     size = {\u001b[33m\"\u001b[39m\u001b[33mshortest_edge\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m56\u001b[39m * \u001b[32m56\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlongest_edge\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m28\u001b[39m * \u001b[32m28\u001b[39m * \u001b[32m1280\u001b[39m}\n",
      "\u001b[31mValueError\u001b[39m: size must contain 'shortest_edge' and 'longest_edge' keys."
     ]
    }
   ],
   "source": [
    "# Reference \n",
    "\n",
    "import time\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "# Suppress specific deprecation warnings from optimum implementation of numpy arrays\n",
    "# This block prevents clogging the API logs \n",
    "warnings.filterwarnings(\"ignore\", message=\"__array__ implementation doesn't accept a copy keyword\")\n",
    "\n",
    "\n",
    "model_id = \"/mnt/Ironwolf-4TB/Models/Pytorch/Qwen2.5-VL-7B-Instruct-int4_sym-ov\"\n",
    "\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\"}\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id, export=False, device=\"GPU.2\", ov_config=ov_config) #trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "image_path = \"dedication.png\"\n",
    "image = Image.open(image_path)\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"image\": image  # The image object is passed here, not just declared as a type\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"<image>\\nDescribe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Print number of tokens\n",
    "# print(f\"Input token length: {len(inputs.input_ids[0])}\")\n",
    "\n",
    "# Inference: Generation of the output with performance metrics\n",
    "start_time = time.time()\n",
    "output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "generated_ids = [output_ids[len(input_ids) :] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Calculate tokens per second\n",
    "num_tokens_generated = len(generated_ids[0])\n",
    "tokens_per_second = num_tokens_generated / generation_time\n",
    "\n",
    "print(f\"Generated text: {output_text}\")\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "print(f\"Tokens generated: {num_tokens_generated}\")\n",
    "print(f\"Speed: {tokens_per_second:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install optimum[openvino]+https://github.com/huggingface/optimum-intel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token length: 265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/echo/anaconda3/envs/OpenArc-Test/lib/python3.11/site-packages/transformers/generation/utils.py:1811: UserWarning: This model does not support `Cache` instances, it only supports the legacy cache format (tuple of tuples). `cache_implementation` (set to hybrid) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "Generation time: 38.18 seconds\n",
      "Tokens generated: 1024\n",
      "Speed: 26.82 tokens/second\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "# Suppress specific deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"__array__ implementation doesn't accept a copy keyword\")\n",
    "\n",
    "model_id = \"/mnt/Ironwolf-4TB/Models/Pytorch/gemma-3-4b-it-int4_asym-ov\"\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\"}\n",
    "# Ensure export=False is correct if the model is already converted\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id, export=False, device=\"GPU.2\", ov_config=ov_config)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "image_path = \"dedication.png\"\n",
    "image = Image.open(image_path)\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "# --- CORRECTED MODIFICATION START ---\n",
    "\n",
    "# 1. Get the correct \"beginning of image\" token from the processor\n",
    "#    This is what the processor internally looks for when matching text and images.\n",
    "image_token = processor.tokenizer.boi_token # Or potentially processor.boi_token if defined directly\n",
    "\n",
    "# 2. Define the text prompt using THIS specific token\n",
    "text_prompt_with_placeholder = f\"{image_token}\\nDescribe this image.\"\n",
    "\n",
    "# 3. Call the processor ONCE, providing both text (with the correct placeholder) and image\n",
    "inputs = processor(\n",
    "    text=[text_prompt_with_placeholder],  # Pass the string with the correct token\n",
    "    images=[image],                       # Pass the PIL image object\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ") # Move inputs to the same device as the model\n",
    "\n",
    "# --- CORRECTED MODIFICATION END ---\n",
    "\n",
    "# Print number of tokens (of the processed input)\n",
    "print(f\"Input token length: {inputs.input_ids.shape[1]}\") # Use shape[1] for tensor length\n",
    "\n",
    "# Inference: Generation of the output with performance metrics\n",
    "start_time = time.time()\n",
    "output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "# Adjust slicing\n",
    "input_ids_len = inputs.input_ids.shape[1]\n",
    "generated_ids = output_ids[:, input_ids_len:] # Correct slicing for tensors\n",
    "\n",
    "# Post-processing\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Calculate tokens per second\n",
    "num_tokens_generated = len(generated_ids[0])\n",
    "tokens_per_second = num_tokens_generated / generation_time if generation_time > 0 else 0\n",
    "\n",
    "# Join the list of strings into a single string if needed\n",
    "final_output_text = \"\".join(output_text)\n",
    "\n",
    "print(f\"Generated text: {final_output_text}\")\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "print(f\"Tokens generated: {num_tokens_generated}\")\n",
    "print(f\"Speed: {tokens_per_second:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token length: 273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/echo/anaconda3/envs/OpenArc-Test/lib/python3.11/site-packages/transformers/generation/utils.py:1811: UserWarning: This model does not support `Cache` instances, it only supports the legacy cache format (tuple of tuples). `cache_implementation` (set to hybrid) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: ['']\n"
     ]
    }
   ],
   "source": [
    "# working\n",
    "\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "# Suppress specific deprecation warnings from optimum implementation of numpy arrays\n",
    "# This block prevents clogging the API logs \n",
    "warnings.filterwarnings(\"ignore\", message=\"__array__ implementation doesn't accept a copy keyword\")\n",
    "\n",
    "\n",
    "model_id = \"/mnt/Ironwolf-4TB/Models/Pytorch/gemma-3-4b-it-int4_asym-ov\"\n",
    "\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\"}\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id, export=False, device=\"GPU.1\", ov_config=ov_config)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "image_path = \"dedication.png\"\n",
    "image = Image.open(image_path)\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Print tokenizer length\n",
    "print(f\"Input token length: {len(inputs.input_ids[0])}\")\n",
    "\n",
    "# Generate output\n",
    "output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "\n",
    "generated_ids = [output_ids[len(input_ids) :] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "print(f\"Generated text: {output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "image_path = \"dedication.png\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Convert image to base64\n",
    "buffered = io.BytesIO()\n",
    "image.save(buffered, format=\"PNG\")\n",
    "img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "# Print the base64 encoding\n",
    "print(f\"Base64 encoded image: {img_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference \n",
    "\n",
    "import time\n",
    "import warnings\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "# Suppress specific deprecation warnings from optimum implementation of numpy arrays\n",
    "# This block prevents clogging the API logs \n",
    "warnings.filterwarnings(\"ignore\", message=\"__array__ implementation doesn't accept a copy keyword\")\n",
    "\n",
    "\n",
    "model_id = \"/mnt/Ironwolf-4TB/Models/OpenVINO/Qwen2.5-VL-3B-Instruct-int4_sym-ov\"\n",
    "\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\"}\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id, export=False, device=\"GPU.1\", ov_config=ov_config)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# Example base64 encoded image (in a real scenario, this would come from the request)\n",
    "image_path = \"dedication.png\"\n",
    "with open(image_path, \"rb\") as img_file:\n",
    "    img_base64 = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "# Create conversation with base64 image\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                }\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extract and decode the base64 image from the conversation\n",
    "images = []\n",
    "for message in conversation:\n",
    "    if message[\"role\"] == \"user\":\n",
    "        for content_item in message[\"content\"]:\n",
    "            if content_item.get(\"type\") == \"image\" and \"image_url\" in content_item:\n",
    "                # Extract base64 data from the URL\n",
    "                image_url = content_item[\"image_url\"][\"url\"]\n",
    "                if image_url.startswith(\"data:\"):\n",
    "                    # Parse the base64 data\n",
    "                    base64_data = image_url.split(\",\")[1] if \",\" in image_url else image_url.split(\";base64,\")[1]\n",
    "                    # Convert base64 to image\n",
    "                    image_data = base64.b64decode(base64_data)\n",
    "                    image = Image.open(BytesIO(image_data))\n",
    "                    images.append(image)\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "inputs = processor(text=[text_prompt], images=images, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Print tokenizer length\n",
    "print(f\"Input token length: {len(inputs.input_ids[0])}\")\n",
    "\n",
    "# Inference: Generation of the output with performance metrics\n",
    "start_time = time.time()\n",
    "output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "generated_ids = [output_ids[len(input_ids) :] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Calculate tokens per second\n",
    "num_tokens_generated = len(generated_ids[0])\n",
    "tokens_per_second = num_tokens_generated / generation_time\n",
    "\n",
    "print(f\"Generated text: {output_text}\")\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "print(f\"Tokens generated: {num_tokens_generated}\")\n",
    "print(f\"Speed: {tokens_per_second:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "import openvino as ov\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "# Define dynamic input shapes (batch, sequence length)\n",
    "input_shape = [1, 128]  # Example: batch=1, seq_len=128\n",
    "dummy_input = torch.randint(0, 100, input_shape)\n",
    "\n",
    "# Convert directly to OpenVINO IR (no ONNX needed!)\n",
    "ov_model = ov.convert_model(\n",
    "    model, \n",
    "    input=[input_shape],  # Supports dynamic axes like [1, \"seq_len\"]\n",
    "    share_weights=True,   # Reduces memory footprint\n",
    ")\n",
    "\n",
    "# Save IR (xml + bin)\n",
    "ov.save_model(ov_model, \"bert_ir.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference \n",
    "\n",
    "import time\n",
    "import warnings\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "# Suppress specific deprecation warnings from optimum implementation of numpy arrays\n",
    "# This block prevents clogging the API logs \n",
    "warnings.filterwarnings(\"ignore\", message=\"__array__ implementation doesn't accept a copy keyword\")\n",
    "\n",
    "\n",
    "model_id = \"/mnt/Ironwolf-4TB/Models/OpenVINO/Qwen2.5-VL-3B-Instruct-int4_sym-ov\"\n",
    "\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\"}\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id, export=False, device=\"GPU.1\", ov_config=ov_config)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# Example base64 encoded image (in a real scenario, this would come from the request)\n",
    "image_path = \"dedication.png\"\n",
    "with open(image_path, \"rb\") as img_file:\n",
    "    img_base64 = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "# Create conversation with base64 image\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                }\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extract and decode the base64 image from the conversation\n",
    "images = []\n",
    "for message in conversation:\n",
    "    if message[\"role\"] == \"user\":\n",
    "        for content_item in message[\"content\"]:\n",
    "            if content_item.get(\"type\") == \"image\" and \"image_url\" in content_item:\n",
    "                # Extract base64 data from the URL\n",
    "                image_url = content_item[\"image_url\"][\"url\"]\n",
    "                if image_url.startswith(\"data:\"):\n",
    "                    # Parse the base64 data\n",
    "                    base64_data = image_url.split(\",\")[1] if \",\" in image_url else image_url.split(\";base64,\")[1]\n",
    "                    # Convert base64 to image\n",
    "                    image_data = base64.b64decode(base64_data)\n",
    "                    image = Image.open(BytesIO(image_data))\n",
    "                    images.append(image)\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "inputs = processor(text=[text_prompt], images=images, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Print tokenizer length\n",
    "print(f\"Input token length: {len(inputs.input_ids[0])}\")\n",
    "\n",
    "# Inference: Generation of the output with performance metrics\n",
    "start_time = time.time()\n",
    "output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "generated_ids = [output_ids[len(input_ids) :] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Calculate tokens per second\n",
    "num_tokens_generated = len(generated_ids[0])\n",
    "tokens_per_second = num_tokens_generated / generation_time\n",
    "\n",
    "print(f\"Generated text: {output_text}\")\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "print(f\"Tokens generated: {num_tokens_generated}\")\n",
    "print(f\"Speed: {tokens_per_second:.2f} tokens/second\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenArc-Test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
