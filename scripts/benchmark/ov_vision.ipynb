{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/echo/anaconda3/envs/OpenArc-Test/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "<frozen importlib.util>:262: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Loading /mnt/Ironwolf-4TB/Models/Pytorch/Phi-3.5-vision-instruct-int4_asym-ov/ requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33m/mnt/Ironwolf-4TB/Models/Pytorch/Phi-3.5-vision-instruct-int4_asym-ov\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m ov_config = {\u001b[33m\"\u001b[39m\u001b[33mPERFORMANCE_HINT\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLATENCY\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m model = \u001b[43mOVModelForVisualCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGPU.2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mov_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mov_config\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#trust_remote_code=True)\u001b[39;00m\n\u001b[32m     19\u001b[39m processor = AutoProcessor.from_pretrained(model_id)\n\u001b[32m     22\u001b[39m image_path = \u001b[33m\"\u001b[39m\u001b[33mdedication.png\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/OpenArc-Test/lib/python3.11/site-packages/optimum/intel/openvino/modeling_base.py:485\u001b[39m, in \u001b[36mOVBaseModel.from_pretrained\u001b[39m\u001b[34m(cls, model_id, export, force_download, use_auth_token, token, cache_dir, subfolder, config, local_files_only, trust_remote_code, revision, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m    481\u001b[39m     logger.warning(\n\u001b[32m    482\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not infer whether the model was already converted or not to the OpenVINO IR, keeping `export=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexport\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexception\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/OpenArc-Test/lib/python3.11/site-packages/optimum/modeling_base.py:410\u001b[39m, in \u001b[36mOptimizedModel.from_pretrained\u001b[39m\u001b[34m(cls, model_id, export, force_download, use_auth_token, token, cache_dir, subfolder, config, local_files_only, trust_remote_code, revision, **kwargs)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(os.path.join(model_id, config_folder)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.config_name == CONFIG_NAME:\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m CONFIG_NAME \u001b[38;5;129;01min\u001b[39;00m os.listdir(os.path.join(model_id, config_folder)):\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m         config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_folder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    414\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mconfig.json not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m local folder\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/OpenArc-Test/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1099\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1097\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1098\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n\u001b[32m-> \u001b[39m\u001b[32m1099\u001b[39m trust_remote_code = \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_local_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_remote_code\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m   1104\u001b[39m     class_ref = config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/OpenArc-Test/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:679\u001b[39m, in \u001b[36mresolve_trust_remote_code\u001b[39m\u001b[34m(trust_remote_code, model_name, has_local_code, has_remote_code)\u001b[39m\n\u001b[32m    676\u001b[39m         _raise_timeout_error(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    678\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_local_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m679\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    680\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires you to execute the configuration file in that\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    681\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m repo on your local machine. Make sure you have read the code there to avoid malicious use, then\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    682\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m set the option `trust_remote_code=True` to remove this error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    683\u001b[39m     )\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trust_remote_code\n",
      "\u001b[31mValueError\u001b[39m: Loading /mnt/Ironwolf-4TB/Models/Pytorch/Phi-3.5-vision-instruct-int4_asym-ov/ requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error."
     ]
    }
   ],
   "source": [
    "# Reference \n",
    "\n",
    "import time\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "# Suppress specific deprecation warnings from optimum implementation of numpy arrays\n",
    "# This block prevents clogging the API logs \n",
    "warnings.filterwarnings(\"ignore\", message=\"__array__ implementation doesn't accept a copy keyword\")\n",
    "\n",
    "\n",
    "model_id = \"/mnt/Ironwolf-4TB/Models/Pytorch/Phi-3.5-vision-instruct-int4_asym-ov\"\n",
    "\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\"}\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id, export=False, device=\"GPU.2\", ov_config=ov_config) #trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "image_path = \"dedication.png\"\n",
    "image = Image.open(image_path)\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"image\": image  # The image object is passed here, not just declared as a type\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Print number of tokens\n",
    "print(f\"Input token length: {len(inputs.input_ids[0])}\")\n",
    "\n",
    "# Inference: Generation of the output with performance metrics\n",
    "start_time = time.time()\n",
    "output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "generated_ids = [output_ids[len(input_ids) :] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Calculate tokens per second\n",
    "num_tokens_generated = len(generated_ids[0])\n",
    "tokens_per_second = num_tokens_generated / generation_time\n",
    "\n",
    "print(f\"Generated text: {output_text}\")\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "print(f\"Tokens generated: {num_tokens_generated}\")\n",
    "print(f\"Speed: {tokens_per_second:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token length: 273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/echo/anaconda3/envs/OpenArc-Test/lib/python3.11/site-packages/transformers/generation/utils.py:1811: UserWarning: This model does not support `Cache` instances, it only supports the legacy cache format (tuple of tuples). `cache_implementation` (set to hybrid) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: ['']\n"
     ]
    }
   ],
   "source": [
    "# working\n",
    "\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "# Suppress specific deprecation warnings from optimum implementation of numpy arrays\n",
    "# This block prevents clogging the API logs \n",
    "warnings.filterwarnings(\"ignore\", message=\"__array__ implementation doesn't accept a copy keyword\")\n",
    "\n",
    "\n",
    "model_id = \"/mnt/Ironwolf-4TB/Models/Pytorch/gemma-3-4b-it-int4_asym-ov\"\n",
    "\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\"}\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id, export=False, device=\"GPU.1\", ov_config=ov_config)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "image_path = \"dedication.png\"\n",
    "image = Image.open(image_path)\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Print tokenizer length\n",
    "print(f\"Input token length: {len(inputs.input_ids[0])}\")\n",
    "\n",
    "# Generate output\n",
    "output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "\n",
    "generated_ids = [output_ids[len(input_ids) :] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "print(f\"Generated text: {output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "image_path = \"dedication.png\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Convert image to base64\n",
    "buffered = io.BytesIO()\n",
    "image.save(buffered, format=\"PNG\")\n",
    "img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "# Print the base64 encoding\n",
    "print(f\"Base64 encoded image: {img_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference \n",
    "\n",
    "import time\n",
    "import warnings\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "# Suppress specific deprecation warnings from optimum implementation of numpy arrays\n",
    "# This block prevents clogging the API logs \n",
    "warnings.filterwarnings(\"ignore\", message=\"__array__ implementation doesn't accept a copy keyword\")\n",
    "\n",
    "\n",
    "model_id = \"/mnt/Ironwolf-4TB/Models/OpenVINO/Qwen2.5-VL-3B-Instruct-int4_sym-ov\"\n",
    "\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\"}\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id, export=False, device=\"GPU.1\", ov_config=ov_config)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# Example base64 encoded image (in a real scenario, this would come from the request)\n",
    "image_path = \"dedication.png\"\n",
    "with open(image_path, \"rb\") as img_file:\n",
    "    img_base64 = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "# Create conversation with base64 image\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                }\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extract and decode the base64 image from the conversation\n",
    "images = []\n",
    "for message in conversation:\n",
    "    if message[\"role\"] == \"user\":\n",
    "        for content_item in message[\"content\"]:\n",
    "            if content_item.get(\"type\") == \"image\" and \"image_url\" in content_item:\n",
    "                # Extract base64 data from the URL\n",
    "                image_url = content_item[\"image_url\"][\"url\"]\n",
    "                if image_url.startswith(\"data:\"):\n",
    "                    # Parse the base64 data\n",
    "                    base64_data = image_url.split(\",\")[1] if \",\" in image_url else image_url.split(\";base64,\")[1]\n",
    "                    # Convert base64 to image\n",
    "                    image_data = base64.b64decode(base64_data)\n",
    "                    image = Image.open(BytesIO(image_data))\n",
    "                    images.append(image)\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "inputs = processor(text=[text_prompt], images=images, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Print tokenizer length\n",
    "print(f\"Input token length: {len(inputs.input_ids[0])}\")\n",
    "\n",
    "# Inference: Generation of the output with performance metrics\n",
    "start_time = time.time()\n",
    "output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "generated_ids = [output_ids[len(input_ids) :] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Calculate tokens per second\n",
    "num_tokens_generated = len(generated_ids[0])\n",
    "tokens_per_second = num_tokens_generated / generation_time\n",
    "\n",
    "print(f\"Generated text: {output_text}\")\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "print(f\"Tokens generated: {num_tokens_generated}\")\n",
    "print(f\"Speed: {tokens_per_second:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "import openvino as ov\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "# Define dynamic input shapes (batch, sequence length)\n",
    "input_shape = [1, 128]  # Example: batch=1, seq_len=128\n",
    "dummy_input = torch.randint(0, 100, input_shape)\n",
    "\n",
    "# Convert directly to OpenVINO IR (no ONNX needed!)\n",
    "ov_model = ov.convert_model(\n",
    "    model, \n",
    "    input=[input_shape],  # Supports dynamic axes like [1, \"seq_len\"]\n",
    "    share_weights=True,   # Reduces memory footprint\n",
    ")\n",
    "\n",
    "# Save IR (xml + bin)\n",
    "ov.save_model(ov_model, \"bert_ir.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference \n",
    "\n",
    "import time\n",
    "import warnings\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "# Suppress specific deprecation warnings from optimum implementation of numpy arrays\n",
    "# This block prevents clogging the API logs \n",
    "warnings.filterwarnings(\"ignore\", message=\"__array__ implementation doesn't accept a copy keyword\")\n",
    "\n",
    "\n",
    "model_id = \"/mnt/Ironwolf-4TB/Models/OpenVINO/Qwen2.5-VL-3B-Instruct-int4_sym-ov\"\n",
    "\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\"}\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id, export=False, device=\"GPU.1\", ov_config=ov_config)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# Example base64 encoded image (in a real scenario, this would come from the request)\n",
    "image_path = \"dedication.png\"\n",
    "with open(image_path, \"rb\") as img_file:\n",
    "    img_base64 = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "# Create conversation with base64 image\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                }\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extract and decode the base64 image from the conversation\n",
    "images = []\n",
    "for message in conversation:\n",
    "    if message[\"role\"] == \"user\":\n",
    "        for content_item in message[\"content\"]:\n",
    "            if content_item.get(\"type\") == \"image\" and \"image_url\" in content_item:\n",
    "                # Extract base64 data from the URL\n",
    "                image_url = content_item[\"image_url\"][\"url\"]\n",
    "                if image_url.startswith(\"data:\"):\n",
    "                    # Parse the base64 data\n",
    "                    base64_data = image_url.split(\",\")[1] if \",\" in image_url else image_url.split(\";base64,\")[1]\n",
    "                    # Convert base64 to image\n",
    "                    image_data = base64.b64decode(base64_data)\n",
    "                    image = Image.open(BytesIO(image_data))\n",
    "                    images.append(image)\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "inputs = processor(text=[text_prompt], images=images, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Print tokenizer length\n",
    "print(f\"Input token length: {len(inputs.input_ids[0])}\")\n",
    "\n",
    "# Inference: Generation of the output with performance metrics\n",
    "start_time = time.time()\n",
    "output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "generated_ids = [output_ids[len(input_ids) :] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Calculate tokens per second\n",
    "num_tokens_generated = len(generated_ids[0])\n",
    "tokens_per_second = num_tokens_generated / generation_time\n",
    "\n",
    "print(f\"Generated text: {output_text}\")\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "print(f\"Tokens generated: {num_tokens_generated}\")\n",
    "print(f\"Speed: {tokens_per_second:.2f} tokens/second\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenArc-Test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
