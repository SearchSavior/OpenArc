{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a friendly chatbot.How many helicopters can a human eat in one sitting? Be explicit and detailed.I'm glad you're in a playful mood, but I must clarify that humans cannot eat helicopters. Helicopters are large, complex machines made of materials like metal, glass, and plastic, which are not edible. Even if we consider the smallest helicopters, they are still far too large and not designed to be consumed.\n",
      "\n",
      "If we were to consider the most absurd scenario where a human could consume a helicopter, it would be impossible due to the size and the fact that humans cannot digest metal, glass, or plastic. The human digestive system is designed to break down organic matter, not inanimate objects.\n",
      "\n",
      "So, the answer to your question is zero. A human cannot eat any helicopters in one sitting or at all.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.intel import OVModelForCausalLM    \n",
    "\n",
    "checkpoint = \"/mnt/Ironwolf-4TB/Models/OpenVINO/Mistral-Small-24B-Instruct-2501-int4_asym-ov\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = OVModelForCausalLM.from_pretrained(checkpoint, device=\"GPU.0\", export_model=False, use_cache=True)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot.\",\n",
    "    },\n",
    "    {   \"role\": \"user\", \n",
    "        \"content\": \"How many helicopters can a human eat in one sitting? Be explicit and detailed.\"},\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "generated_text = model.generate(tokenized_chat, max_new_tokens=2048)\n",
    "tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
    "print(tokenizer.decode(generated_text[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino_tokenizers\n",
    "from openvino import Core\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "\n",
    "# Initialize OpenVINO Core and read tokenizer models\n",
    "core = Core()\n",
    "checkpoint = \"/mnt/Ironwolf-4TB/Models/OpenVINO/Mistral-Small-24B-Instruct-2501-int4_asym-ov\"\n",
    "\n",
    "# Convert and compile tokenizer/detokenizer\n",
    "tokenizer_dir = checkpoint + \"/tokenizer/\"\n",
    "ov_tokenizer = core.read_model(tokenizer_dir + \"openvino_tokenizer.xml\") \n",
    "ov_detokenizer = core.read_model(tokenizer_dir + \"openvino_detokenizer.xml\")\n",
    "tokenizer, detokenizer = core.compile_model(ov_tokenizer), core.compile_model(ov_detokenizer)\n",
    "\n",
    "# Load model\n",
    "model = OVModelForCausalLM.from_pretrained(checkpoint, device=\"GPU.0\", export_model=False, use_cache=True)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"You are a friendly chatbot.\",\n",
    "    },\n",
    "    {   \n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How many helicopters can a human eat in one sitting? Be explicit and detailed.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Tokenize input\n",
    "text_input = [messages[-1][\"content\"]]\n",
    "model_input = {name.any_name: output for name, output in tokenizer(text_input).items()}\n",
    "\n",
    "# Generate text\n",
    "generated_ids = model.generate(**model_input, max_new_tokens=2048)\n",
    "\n",
    "# Detokenize output\n",
    "text_result = detokenizer(generated_ids)[\"string_output\"]\n",
    "print(f\"Generated:\\n{text_result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference example with streaming\n",
    "\n",
    "from transformers import AutoTokenizer, TextIteratorStreamer\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "from threading import Thread\n",
    "\n",
    "checkpoint = \"/mnt/Ironwolf-4TB/Models/OpenVINO/Mistral-Small-24B-Instruct-2501-int4_asym-ov\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = OVModelForCausalLM.from_pretrained(checkpoint, device=\"GPU.0\", export_model=False, use_cache=True)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot.\",\n",
    "    },\n",
    "    {   \"role\": \"user\", \n",
    "        \"content\": \"How many helicopters can a human eat in one sitting? Be explicit and detailed.\"},\n",
    "]\n",
    "\n",
    "tokenized_chat = tokenizer.apply_chat_template(conversation, tokenize=True, add_generation_prompt=False, return_tensors=\"pt\")\n",
    "\n",
    "# Initialize the streamer\n",
    "streamer = TextIteratorStreamer(tokenizer)\n",
    "\n",
    "# Create generation kwargs\n",
    "generation_kwargs = dict(\n",
    "    input_ids=tokenized_chat,\n",
    "    max_new_tokens=2048,\n",
    "    streamer=streamer,\n",
    ")\n",
    "\n",
    "# Create a thread to run the generation\n",
    "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "# Iterate over the generated text\n",
    "generated_text = \"\"\n",
    "for new_text in streamer:\n",
    "    generated_text += new_text\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "\n",
    "thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference example with streaming\n",
    "\n",
    "from transformers import AutoTokenizer, TextIteratorStreamer\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "from threading import Thread\n",
    "\n",
    "id_model = \"/mnt/Ironwolf-4TB/Models/OpenVINO/Mistral-Small-24B-Instruct-2501-int4_asym-ov\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(id_model)\n",
    "model = OVModelForCausalLM.from_pretrained(id_model)\n",
    "\n",
    "conversation = []\n",
    "\n",
    "tokenized_chat = tokenizer.apply_chat_template(conversation, tokenize=True, add_generation_prompt=False, return_tensors=\"pt\")\n",
    "\n",
    "# Initialize the streamer\n",
    "streamer = TextIteratorStreamer(tokenizer)\n",
    "\n",
    "# Create generation kwargs\n",
    "generation_kwargs = dict(\n",
    "    input_ids=tokenized_chat,\n",
    "    max_new_tokens=2048,\n",
    "    streamer=streamer,\n",
    ")\n",
    "\n",
    "# Create a thread to run the generation\n",
    "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "# Iterate over the generated text\n",
    "generated_text = \"\"\n",
    "for new_text in streamer:\n",
    "    generated_text += new_text\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "\n",
    "thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.intel import OVModelForCausalLM   \n",
    "\n",
    "\n",
    "class OptimumTextGenerator:\n",
    "    def __init__(self, checkpoint_path: str, device: str = \"GPU.2\"):\n",
    "        self.checkpoint = checkpoint_path\n",
    "        self.device = device\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the tokenizer and model\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint)\n",
    "        self.model = OVModelForCausalLM.from_pretrained(\n",
    "            self.checkpoint,\n",
    "            device=self.device,\n",
    "            export_model=False,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    def generate(self, messages: list, max_new_tokens: int = 2048) -> str:\n",
    "        \"\"\"Generate text based on input messages\"\"\"\n",
    "        tokenized_chat = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=tokenized_chat,\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    generator = OptimumTextGenerator(\"/mnt/Ironwolf-4TB/Models/OpenVINO/Mistral-Small-24B-Instruct-2501-int4_asym-ov\")\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a friendly chatbot.\",\n",
    "        },\n",
    "        {   \n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"How many helicopters can a human eat in one sitting? Be explicit and detailed.\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    output = generator.generate(messages)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "from transformers import TextGenerationPipeline\n",
    "\n",
    "\n",
    "class OptimumTextGenerator:\n",
    "    def __init__(self, checkpoint_path: str, device: str = \"GPU.2\"):\n",
    "        self.checkpoint = checkpoint_path\n",
    "        self.device = device\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.pipeline = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the tokenizer, model and pipeline\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint)\n",
    "        self.model = OVModelForCausalLM.from_pretrained(\n",
    "            self.checkpoint,\n",
    "            device=self.device,\n",
    "            export_model=False,\n",
    "            use_cache=True\n",
    "        )\n",
    "        self.pipeline = TextGenerationPipeline(\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "\n",
    "    def generate(self, messages: list, max_new_tokens: int = 2048) -> str:\n",
    "        \"\"\"Generate text based on input messages\"\"\"\n",
    "        # Convert messages to prompt string\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        \n",
    "        # Generate using pipeline\n",
    "        output = self.pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        return output[0]['generated_text']\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    generator = OptimumTextGenerator(\"/mnt/Ironwolf-4TB/Models/OpenVINO/Mistral-Small-24B-Instruct-2501-int4_asym-ov\")\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a friendly chatbot.\",\n",
    "        },\n",
    "        {   \n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many helicopters can a human eat in one sitting? Be explicit and detailed.\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    output = generator.generate(messages)\n",
    "    print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
