{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/echo/anaconda3/envs/OpenArc_test2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Could not infer whether the model was already converted or not to the OpenVINO IR, keeping `export=False`.\n",
      "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/Ironwolf-4TB/Models/Pytorch/Qwen2.5-VL-7B-Instruct-int4_sym-ov'. Use `repo_type` argument if needed.\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/Ironwolf-4TB/Models/Pytorch/Qwen2.5-VL-7B-Instruct-int4_sym-ov'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33m/mnt/Ironwolf-4TB/Models/Pytorch/Qwen2.5-VL-7B-Instruct-int4_sym-ov\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m ov_config = {\u001b[33m\"\u001b[39m\u001b[33mPERFORMANCE_HINT\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLATENCY\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m model = \u001b[43mOVModelForVisualCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGPU.2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mov_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mov_config\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#trust_remote_code=True)\u001b[39;00m\n\u001b[32m     19\u001b[39m processor = AutoProcessor.from_pretrained(model_id)\n\u001b[32m     22\u001b[39m image_path = \u001b[33m\"\u001b[39m\u001b[33mdedication.png\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/OpenArc_test2/lib/python3.11/site-packages/optimum/intel/openvino/modeling_base.py:498\u001b[39m, in \u001b[36mOVBaseModel.from_pretrained\u001b[39m\u001b[34m(cls, model_id, export, force_download, use_auth_token, token, cache_dir, subfolder, config, local_files_only, trust_remote_code, revision, **kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m    494\u001b[39m     logger.warning(\n\u001b[32m    495\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not infer whether the model was already converted or not to the OpenVINO IR, keeping `export=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexport\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexception\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    496\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m498\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/OpenArc_test2/lib/python3.11/site-packages/optimum/modeling_base.py:386\u001b[39m, in \u001b[36mOptimizedModel.from_pretrained\u001b[39m\u001b[34m(cls, model_id, export, force_download, use_auth_token, token, cache_dir, subfolder, config, local_files_only, trust_remote_code, revision, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m         logger.warning(\n\u001b[32m    382\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe argument `revision` was set to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but will be ignored for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id.split(\u001b[33m'\u001b[39m\u001b[33m@\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    383\u001b[39m         )\n\u001b[32m    384\u001b[39m     model_id, revision = model_id.split(\u001b[33m\"\u001b[39m\u001b[33m@\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m all_files, _ = \u001b[43mTasksManager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_model_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m config_folder = subfolder\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.config_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_files:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/OpenArc_test2/lib/python3.11/site-packages/optimum/exporters/tasks.py:1635\u001b[39m, in \u001b[36mTasksManager.get_model_files\u001b[39m\u001b[34m(model_name_or_path, subfolder, cache_dir, use_auth_token, token, revision)\u001b[39m\n\u001b[32m   1633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_name_or_path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   1634\u001b[39m     model_name_or_path = \u001b[38;5;28mstr\u001b[39m(model_name_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1635\u001b[39m all_files = \u001b[43mhf_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_repo_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1636\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1637\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1638\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1640\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1641\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m subfolder != \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1642\u001b[39m     all_files = [file[\u001b[38;5;28mlen\u001b[39m(subfolder) + \u001b[32m1\u001b[39m :] \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m all_files \u001b[38;5;28;01mif\u001b[39;00m file.startswith(subfolder)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/OpenArc_test2/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mzip\u001b[39m(signature.parameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[32m    103\u001b[39m     kwargs.items(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[32m    104\u001b[39m ):\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    109\u001b[39m         has_token = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/OpenArc_test2/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:154\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m repo_id.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must be in the form \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrepo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnamespace/repo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Use `repo_type` argument if needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m--\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m\u001b[33m are\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m forbidden, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m cannot start or end the name, max length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/Ironwolf-4TB/Models/Pytorch/Qwen2.5-VL-7B-Instruct-int4_sym-ov'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "# Reference \n",
    "\n",
    "import time\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "# Suppress specific deprecation warnings from optimum implementation of numpy arrays\n",
    "# This block prevents clogging the API logs \n",
    "warnings.filterwarnings(\"ignore\", message=\"__array__ implementation doesn't accept a copy keyword\")\n",
    "\n",
    "\n",
    "model_id = \"/mnt/Ironwolf-4TB/Models/Pytorch/Qwen2.5-VL-7B-Instruct-int4_sym-ov\"\n",
    "\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\"}\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id, export=False, device=\"GPU.2\", ov_config=ov_config) #trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "image_path = \"dedication.png\"\n",
    "image = Image.open(image_path)\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"image\": image  # The image object is passed here, not just declared as a type\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"<image>\\nDescribe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Print number of tokens\n",
    "# print(f\"Input token length: {len(inputs.input_ids[0])}\")\n",
    "\n",
    "# Inference: Generation of the output with performance metrics\n",
    "start_time = time.time()\n",
    "output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "generated_ids = [output_ids[len(input_ids) :] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Calculate tokens per second\n",
    "num_tokens_generated = len(generated_ids[0])\n",
    "tokens_per_second = num_tokens_generated / generation_time\n",
    "\n",
    "print(f\"Generated text: {output_text}\")\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "print(f\"Tokens generated: {num_tokens_generated}\")\n",
    "print(f\"Speed: {tokens_per_second:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install optimum[openvino]+https://github.com/huggingface/optimum-intel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token length: 265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/echo/anaconda3/envs/OpenArc-Test/lib/python3.11/site-packages/transformers/generation/utils.py:1811: UserWarning: This model does not support `Cache` instances, it only supports the legacy cache format (tuple of tuples). `cache_implementation` (set to hybrid) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "Generation time: 38.18 seconds\n",
      "Tokens generated: 1024\n",
      "Speed: 26.82 tokens/second\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "# Suppress specific deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"__array__ implementation doesn't accept a copy keyword\")\n",
    "\n",
    "model_id = \"/mnt/Ironwolf-4TB/Models/Pytorch/gemma-3-4b-it-int4_asym-ov\"\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\"}\n",
    "# Ensure export=False is correct if the model is already converted\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id, export=False, device=\"GPU.2\", ov_config=ov_config)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "image_path = \"dedication.png\"\n",
    "image = Image.open(image_path)\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "# --- CORRECTED MODIFICATION START ---\n",
    "\n",
    "# 1. Get the correct \"beginning of image\" token from the processor\n",
    "#    This is what the processor internally looks for when matching text and images.\n",
    "image_token = processor.tokenizer.boi_token # Or potentially processor.boi_token if defined directly\n",
    "\n",
    "# 2. Define the text prompt using THIS specific token\n",
    "text_prompt_with_placeholder = f\"{image_token}\\nDescribe this image.\"\n",
    "\n",
    "# 3. Call the processor ONCE, providing both text (with the correct placeholder) and image\n",
    "inputs = processor(\n",
    "    text=[text_prompt_with_placeholder],  # Pass the string with the correct token\n",
    "    images=[image],                       # Pass the PIL image object\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ") # Move inputs to the same device as the model\n",
    "\n",
    "# --- CORRECTED MODIFICATION END ---\n",
    "\n",
    "# Print number of tokens (of the processed input)\n",
    "print(f\"Input token length: {inputs.input_ids.shape[1]}\") # Use shape[1] for tensor length\n",
    "\n",
    "# Inference: Generation of the output with performance metrics\n",
    "start_time = time.time()\n",
    "output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "# Adjust slicing\n",
    "input_ids_len = inputs.input_ids.shape[1]\n",
    "generated_ids = output_ids[:, input_ids_len:] # Correct slicing for tensors\n",
    "\n",
    "# Post-processing\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Calculate tokens per second\n",
    "num_tokens_generated = len(generated_ids[0])\n",
    "tokens_per_second = num_tokens_generated / generation_time if generation_time > 0 else 0\n",
    "\n",
    "# Join the list of strings into a single string if needed\n",
    "final_output_text = \"\".join(output_text)\n",
    "\n",
    "print(f\"Generated text: {final_output_text}\")\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "print(f\"Tokens generated: {num_tokens_generated}\")\n",
    "print(f\"Speed: {tokens_per_second:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token length: 273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/echo/anaconda3/envs/OpenArc-Test/lib/python3.11/site-packages/transformers/generation/utils.py:1811: UserWarning: This model does not support `Cache` instances, it only supports the legacy cache format (tuple of tuples). `cache_implementation` (set to hybrid) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: ['']\n"
     ]
    }
   ],
   "source": [
    "# working\n",
    "\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "# Suppress specific deprecation warnings from optimum implementation of numpy arrays\n",
    "# This block prevents clogging the API logs \n",
    "warnings.filterwarnings(\"ignore\", message=\"__array__ implementation doesn't accept a copy keyword\")\n",
    "\n",
    "\n",
    "model_id = \"/mnt/Ironwolf-4TB/Models/Pytorch/gemma-3-4b-it-int4_asym-ov\"\n",
    "\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\"}\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id, export=False, device=\"GPU.1\", ov_config=ov_config)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "image_path = \"dedication.png\"\n",
    "image = Image.open(image_path)\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Print tokenizer length\n",
    "print(f\"Input token length: {len(inputs.input_ids[0])}\")\n",
    "\n",
    "# Generate output\n",
    "output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "\n",
    "generated_ids = [output_ids[len(input_ids) :] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "print(f\"Generated text: {output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "image_path = \"dedication.png\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Convert image to base64\n",
    "buffered = io.BytesIO()\n",
    "image.save(buffered, format=\"PNG\")\n",
    "img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "# Print the base64 encoding\n",
    "print(f\"Base64 encoded image: {img_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference \n",
    "\n",
    "import time\n",
    "import warnings\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "# Suppress specific deprecation warnings from optimum implementation of numpy arrays\n",
    "# This block prevents clogging the API logs \n",
    "warnings.filterwarnings(\"ignore\", message=\"__array__ implementation doesn't accept a copy keyword\")\n",
    "\n",
    "\n",
    "model_id = \"/mnt/Ironwolf-4TB/Models/OpenVINO/Qwen2.5-VL-3B-Instruct-int4_sym-ov\"\n",
    "\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\"}\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id, export=False, device=\"GPU.1\", ov_config=ov_config)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# Example base64 encoded image (in a real scenario, this would come from the request)\n",
    "image_path = \"dedication.png\"\n",
    "with open(image_path, \"rb\") as img_file:\n",
    "    img_base64 = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "# Create conversation with base64 image\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                }\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extract and decode the base64 image from the conversation\n",
    "images = []\n",
    "for message in conversation:\n",
    "    if message[\"role\"] == \"user\":\n",
    "        for content_item in message[\"content\"]:\n",
    "            if content_item.get(\"type\") == \"image\" and \"image_url\" in content_item:\n",
    "                # Extract base64 data from the URL\n",
    "                image_url = content_item[\"image_url\"][\"url\"]\n",
    "                if image_url.startswith(\"data:\"):\n",
    "                    # Parse the base64 data\n",
    "                    base64_data = image_url.split(\",\")[1] if \",\" in image_url else image_url.split(\";base64,\")[1]\n",
    "                    # Convert base64 to image\n",
    "                    image_data = base64.b64decode(base64_data)\n",
    "                    image = Image.open(BytesIO(image_data))\n",
    "                    images.append(image)\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "inputs = processor(text=[text_prompt], images=images, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Print tokenizer length\n",
    "print(f\"Input token length: {len(inputs.input_ids[0])}\")\n",
    "\n",
    "# Inference: Generation of the output with performance metrics\n",
    "start_time = time.time()\n",
    "output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "generated_ids = [output_ids[len(input_ids) :] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Calculate tokens per second\n",
    "num_tokens_generated = len(generated_ids[0])\n",
    "tokens_per_second = num_tokens_generated / generation_time\n",
    "\n",
    "print(f\"Generated text: {output_text}\")\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "print(f\"Tokens generated: {num_tokens_generated}\")\n",
    "print(f\"Speed: {tokens_per_second:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "import openvino as ov\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "# Define dynamic input shapes (batch, sequence length)\n",
    "input_shape = [1, 128]  # Example: batch=1, seq_len=128\n",
    "dummy_input = torch.randint(0, 100, input_shape)\n",
    "\n",
    "# Convert directly to OpenVINO IR (no ONNX needed!)\n",
    "ov_model = ov.convert_model(\n",
    "    model, \n",
    "    input=[input_shape],  # Supports dynamic axes like [1, \"seq_len\"]\n",
    "    share_weights=True,   # Reduces memory footprint\n",
    ")\n",
    "\n",
    "# Save IR (xml + bin)\n",
    "ov.save_model(ov_model, \"bert_ir.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token length: 547\n",
      "Generated text: ['The image is a page from a book or document, formatted in a traditional serif font. The top of the page has a heading that reads \"Dedication\" followed by the number \"xxiii,\" indicating it is the 23rd dedication in the text. Below the heading, there is a paragraph discussing the use of the word \"law\" in two different senses and contrasting it with the laws based on rewards and punishments as opposed to laws derived from phenomena. The text mentions figures like Hooker and Montesquieu and discusses the implications of using \"law\" in these ways. The bottom of the page includes a note that the page was digitized by Google and that the original file is held at Harvard University. The page appears to be part of a larger text, possibly a philosophical or scientific work, given the nature of the discussion about law and its applications.']\n",
      "Generation time: 9.76 seconds\n",
      "Tokens generated: 177\n",
      "Speed: 18.14 tokens/second\n"
     ]
    }
   ],
   "source": [
    "# Reference \n",
    "\n",
    "import time\n",
    "import warnings\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "# Suppress specific deprecation warnings from optimum implementation of numpy arrays\n",
    "# This block prevents clogging the API logs \n",
    "warnings.filterwarnings(\"ignore\", message=\"__array__ implementation doesn't accept a copy keyword\")\n",
    "\n",
    "\n",
    "model_id = \"/mnt/Ironwolf-4TB/Models/OpenVINO/Qwen2.5-VL-3B-Instruct-int4_sym-ov\"\n",
    "\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\"}\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id, export=False, device=\"GPU.1\", ov_config=ov_config)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# Example base64 encoded image (in a real scenario, this would come from the request)\n",
    "image_path = \"dedication.png\"\n",
    "with open(image_path, \"rb\") as img_file:\n",
    "    img_base64 = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "# Create conversation with base64 image\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                }\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extract and decode the base64 image from the conversation\n",
    "images = []\n",
    "for message in conversation:\n",
    "    if message[\"role\"] == \"user\":\n",
    "        for content_item in message[\"content\"]:\n",
    "            if content_item.get(\"type\") == \"image\" and \"image_url\" in content_item:\n",
    "                # Extract base64 data from the URL\n",
    "                image_url = content_item[\"image_url\"][\"url\"]\n",
    "                if image_url.startswith(\"data:\"):\n",
    "                    # Parse the base64 data\n",
    "                    base64_data = image_url.split(\",\")[1] if \",\" in image_url else image_url.split(\";base64,\")[1]\n",
    "                    # Convert base64 to image\n",
    "                    image_data = base64.b64decode(base64_data)\n",
    "                    image = Image.open(BytesIO(image_data))\n",
    "                    images.append(image)\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "inputs = processor(text=[text_prompt], images=images, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Print tokenizer length\n",
    "print(f\"Input token length: {len(inputs.input_ids[0])}\")\n",
    "\n",
    "# Inference: Generation of the output with performance metrics\n",
    "start_time = time.time()\n",
    "output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "generated_ids = [output_ids[len(input_ids) :] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Calculate tokens per second\n",
    "num_tokens_generated = len(generated_ids[0])\n",
    "tokens_per_second = num_tokens_generated / generation_time\n",
    "\n",
    "print(f\"Generated text: {output_text}\")\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "print(f\"Tokens generated: {num_tokens_generated}\")\n",
    "print(f\"Speed: {tokens_per_second:.2f} tokens/second\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
