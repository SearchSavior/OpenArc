

from pathlib import Path
import gradio as gr
import json


class Optimum_Loader:
    def __init__(self, payload_constructor):
        self.payload_constructor = payload_constructor
        self.components = {}

    def read_openvino_config(self, id_model):
        """Read the OpenVINO config file from the model directory and display it in the dashboard as a JSON object.
        This file is generated by the OpenVINO toolkit and contains metadata about the model's configuration and optimizations based on
        how it was converted to the OpenVINO IR.
        """
        try:
            # Convert to Path object and get parent directory
            model_path = Path(id_model)
            config_path = model_path / "openvino_config.json"
            
            if config_path.exists():
                return json.loads(config_path.read_text())
            return {"message": f"No openvino_config.json found in {str(config_path)}"}
        except Exception as e:
            return {"error": f"Error reading config: {str(e)}"}

    def read_architecture(self, id_model):
        """Read the architecture file from the model directory and display it in the dashboard as a JSON object.
        While not explicitly required for inference, this file contains metadata about the model's architecture 
        and can be useful for debugging performance by understanding the model's structure to choose optimization parameters.
        """
        try:
            # Convert to Path object and get parent directory
            model_path = Path(id_model)
            architecture_path = model_path / "config.json"
            
            if architecture_path.exists():
                return json.loads(architecture_path.read_text())
            return {"message": f"No architecture.json found in {str(architecture_path)}"}
        except Exception as e:
            return {"error": f"Error reading architecture: {str(e)}"}
        
    def read_generation_config(self, id_model):
        """Read the generation config file from the model directory and display it in the dashboard as a JSON object.
        This file contains the ground truth of what sampling parameters should be used 
        for inference. These values are dervied directly from the model's pytorch metadata and should be taken as precedent for benchmararks.
        """
        try:
            model_path = Path(id_model)
            generation_config_path = model_path / "generation_config.json"
            
            if generation_config_path.exists():
                return json.loads(generation_config_path.read_text())
            return {"message": f"No generation_config.json found in {str(generation_config_path)}"}
        except Exception as e:
            return {"error": f"Error reading generation config: {str(e)}"}  
        
    def read_tokenizer_config(self, id_model):
        """Read the tokenizer config file from the model directory and display it in the dashboard as a JSON object.
        It might be overkill to include this file in the dashboard, but it's included for completeness.
        OpenArc's goal is to be as flexible as working inside of a script; in that inference scenario you would invedtigate these values and hardcode them.
        Therefore it would not matter if the tokenizer config matches a standardized format for different models. To make OpenArc more scalable, we account for this.
        """
        try:
            model_path = Path(id_model)
            tokenizer_config_path = model_path / "tokenizer_config.json"    
            
            if tokenizer_config_path.exists():
                return json.loads(tokenizer_config_path.read_text())
            return {"message": f"No tokenizer_config.json found in {str(tokenizer_config_path)}"}
        except Exception as e:
            return {"error": f"Error reading tokenizer config: {str(e)}"}   
        
    def create_interface(self):
        with gr.Tab("Loader"):
            with gr.Row():
                self.load_model_interface()
                self.debug_tool()
                self.setup_button_handlers()

    def load_model_interface(self):
        with gr.Column(min_width=500, scale=1):
            # Model Basic Configuration
            with gr.Group("Model Configuration"):
                self.components.update({
                    'id_model': gr.Textbox(
                        label="Model Identifier or Path",
                        placeholder="Enter model identifier or local path",
                        info="Enter the model's Hugging Face identifier or local path"
                    ),
                    'device': gr.Dropdown(
                        choices=["", "AUTO", "CPU", "GPU.0", "GPU.1", "GPU.2", "AUTO:GPU.0,GPU.1", "AUTO:GPU.0,GPU.1,GPU.2"],
                        label="Device",
                        value="",
                        info="Select the device for model inference"
                    ),
                    'use_cache': gr.Checkbox(
                        label="Use Cache",
                        value=True,
                        info="Enable cache for stateful models (disable for multi-GPU)"
                    ),
                    'export_model': gr.Checkbox(
                        label="Export Model",
                        value=False,
                        info="Whether to export the model to int8_asym. Default and not recommended."
                    ),
                    'dynamic_shapes': gr.Checkbox(
                        label="Dynamic Shapes",
                        value=True,
                        info="Whether to use dynamic shapes. Default is True. Should only be disabled for NPU inference."
                    ),
                    'is_vision_model': gr.Checkbox(
                        label="Is Vision Model",
                        value=False,
                        info="Whether the model is a vision model. Default is False."
                    )
                })

            # Token Configuration
            with gr.Group("Token Settings"):
                self.components.update({
                    'bos_token_id': gr.Textbox(
                        label="bos_token_id",
                        value="",
                        
                    ),
                    'eos_token_id': gr.Textbox(
                        label="eos_token_id",
                        value="",
                        
                    ),
                    'pad_token_id': gr.Textbox(
                        label="pad_token_id",
                        value="",
                        
                    )
                })

            # Performance Optimization
            with gr.Group("Performance Settings"):
                self.components.update({
                    'num_streams': gr.Textbox(
                        label="Number of Streams",
                        value="",
                        placeholder="Leave empty for default",
                        info="Number of inference streams (optional)"
                    ),
                    'performance_hint': gr.Dropdown(
                        choices=["", "LATENCY", "THROUGHPUT", "CUMULATIVE_THROUGHPUT"],
                        label="Performance Hint",
                        value="",
                        info="Select performance optimization strategy"
                    ),
                    'precision_hint': gr.Dropdown(
                        choices=["", "auto", "fp16", "fp32", "int8"],
                        label="Precision Hint",
                        value="",
                        info="Select model precision for computation"
                    ),
                    'enable_hyperthreading': gr.Checkbox(
                        label="Enable Hyperthreading",
                        value=True,
                        info="Enable hyperthreading for CPU inference"
                    ),
                    'inference_num_threads': gr.Textbox(
                        label="Inference Number of Threads",
                        value="",
                        placeholder="Leave empty for default",
                        info="Number of inference threads (optional)"
                    )
                })

            # Action Buttons
            with gr.Row():
                self.components.update({
                    'load_button': gr.Button("Load Model", variant="primary"),
                    'unload_button': gr.Button("Unload Model", variant="secondary"),
                    'status_button': gr.Button("Check Status", variant="secondary")
                })

    def debug_tool(self):
        with gr.Column(min_width=300, scale=1):
            with gr.Accordion("Debug Log", open=True):
                self.components['debug_log'] = gr.JSON(
                    label="Log Output",
                    value={"message": "Debug information will appear here..."},
                )
            with gr.Accordion("OpenVINO Config", open=False):
                self.components['config_viewer'] = gr.JSON(
                    label="OpenVINO Configuration",
                    value={"message": "Config will appear here when model path is entered..."},
                )
            with gr.Accordion("Architecture", open=False):
                self.components['architecture_viewer'] = gr.JSON(
                    label="Architecture",
                    value={"message": "Architecture will appear here when model path is entered..."},
                )

            with gr.Accordion("Generation Config", open=False):
                self.components['generation_config_viewer'] = gr.JSON(
                    label="Generation Configuration",
                    value={"message": "Generation config will appear here when model path is entered..."},
                )   

            with gr.Accordion("Tokenizer Config", open=True):
                self.components['tokenizer_config_viewer'] = gr.JSON(
                    label="Tokenizer Configuration",
                    value={"message": "Tokenizer config will appear here when model path is entered..."},
                )   

    def setup_button_handlers(self):
        self.build_load_request()
        
        # Add handler for model path changes
        self.components['id_model'].change(
            fn=self.read_openvino_config,
            inputs=[self.components['id_model']],
            outputs=[self.components['config_viewer']]
        )

        self.components['id_model'].change(
            fn=self.read_architecture,
            inputs=[self.components['id_model']],
            outputs=[self.components['architecture_viewer']]
        )

        self.components['id_model'].change(
            fn=self.read_generation_config,
            inputs=[self.components['id_model']],
            outputs=[self.components['generation_config_viewer']]
        )

        self.components['id_model'].change(
            fn=self.read_tokenizer_config,
            inputs=[self.components['id_model']],
            outputs=[self.components['tokenizer_config_viewer']]
        )
        
    def build_load_request(self):
        self.components['load_button'].click(
            fn=self.payload_constructor.load_model,
            inputs=[
                self.components[key] for key in [
                    'id_model', 'device', 'use_cache', 'export_model',
                    'num_streams', 'performance_hint', 'precision_hint',
                    'is_vision_model',
                    'bos_token_id', 'eos_token_id', 'pad_token_id',
                    'enable_hyperthreading', 'inference_num_threads', 'dynamic_shapes'
                ]
            ],
            outputs=[self.components['debug_log']]
        )
        
        self.components['unload_button'].click(
            fn=self.payload_constructor.unload_model,
            inputs=None,
            outputs=[self.components['debug_log']]
        )

        self.components['status_button'].click(
            fn=self.payload_constructor.status,
            inputs=None,
            outputs=[self.components['debug_log']]
        )
